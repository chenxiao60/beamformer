<!DOCTYPE html>
<html>
	<head>
 		<title>Sam Rohrer - Senior Tech</title>
  	<meta name="viewport" content="width=device-width, initial-scale=1.0">
  	<bootstrap.min.css>
  	<link href="website/css/bootstrap.css" rel="stylesheet" media="screen">
	<link href="website/css/bootstrap-responsive.css" rel="stylesheet">
	<style>
	body{
		padding-left:-40px;
		padding-right:-40px;
		padding-top:-100px;
	}
    .carousel img {
      position: absolute;
      top: 0;
      left: 0;
      min-width: 100%;
	  height: 500px;
    }

	.carousel-caption {
	      margin-botton: 100px;
	      text-align:center;
	}
	</style>
	</head>

	
	<!-- ********************** Navbar ******************* -->
	<nav class="navbar navbar-inverse navbar-fixed-top" role="navigation">
	  <div class="navbar-header">
	    <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
	      <span class="sr-only">Toggle navigation</span>
	      <span class="icon-bar"></span>
	      <span class="icon-bar"></span>
	      <span class="icon-bar"></span>
	    </button>
	    <a class="navbar-brand" href="#">Home</a>
	  </div>

	  <!-- Collect the nav links, forms, and other content for toggling -->
	  <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
	    <ul class="nav navbar-nav">
	      <li class = ""><a href="#about">About Me</a></li>
	      <li class = ""><a href="#overview">Project Overview</a></li>
		  <li class = ""><a href="#beamforming">Beamforming</a></li>
		  <li class = ""><a href="#simulations">Simulations</a></li>
		  <li class = ""><a href="#fpga_design">FPGA Design</a></li>
		  <li class = ""><a href="#design_files">Design Files</a></li>
	    </ul>  
	  </div><!-- /.navbar-collapse -->
	</nav>

	<!-- *************** Body ***************** --> 
	   
	  <div class="carousel-inner">
		<div class="item active">
          <img style = "padding-top:-40px;" src="pics/background.jpg" alt="">
           <div class="container">
            <div class="carousel-caption">
              <h1>A Novel Beamforming Approach to Stereo Sound</h1>
              <p class="lead">A photo of the completed system undergoing testing.</p>
            </div>
          </div>
        </div>
	   </div>

	<div class ="container">   
	<a name = "about"></a>   
	<hr class="featurette-divider">
	<h2> About Me </h2>
	<p> I am a senior at Thomas Jefferson High School for Science and Technology in the Microelectronics Research Lab. This is my project about beamforming that I've been working on for a few years.</p>
	<br>
	<p> Check out my <a href = "http://srohrer32.github.io">website</a>!</p>
	
	
	<a name = "overview"></a>
    <hr class="featurette-divider">
	<h2> Project Overview </h2>
	<p>The goal of this ongoing project is to determine the best method of creating stereo sound and separation within the footprint of a tablet in the near-field (less than six feet from the user). Beamforming was used to achieve this; however, the best method of near-field beamforming was unknown. Three methods were simulated and compared using MATLAB: point and sum (a new method developed during research), delay and sum beamforming, and no beamforming. Simulation results showed that point and sum beamforming was the superior method. This methodology was then tested in hardware. To do this, the processing was written in VHDL and synthesized onto a FPGA. A speaker enclosure was built to house the five speakers used to create the beamforming array. The outputs of the FPGA were multiplexed into DACs then amplified and played through speakers. Results suggest the system is functioning correctly by separating the sound and many users have confirmed this fact. It is clear that point and sum beamforming is the best method for near-field beamforming. Also, stereo separation can be created in the footprint of a tablet with this method. Applications of this research abound in the fields of medicine, radar and cellular communications.</p>
	
	
	<a name = "beamforming"></a>
    <hr class="featurette-divider">
	<h2> Beamforming </h2>
	<p>This research aims to take a novel approach to creating stereo isolation for a tablet environment within the footprint of the tablet by evaluating the different methods of beamforming and then applying the chosen method to a speaker system. Stereo isolation occurs in headphones as the channels are separated completely, and this isolation is critical to a strong stereo image. Although beamforming has been a part of radar systems for many years, it has only recently seen use in acoustic applications. As of now the two main applications of acoustic beamforming are: sound bars used in home theaters that use some beamforming to create a faux surround sound effect, and large conference room microphones that isolate individual speakers. However, there are no commercial products that utilize beamforming to create stereo isolation within the footprint of a tablet. Furthermore, there is no established research setting out a methodology for attempting beamforming in such a small field of space.</p>

	<p>Currently, there is not a sufficient amount of research into acoustic beamforming techniques. This necessitates reliance on previous research in other fields of beamforming to assess the current state of this field of research. There are many different ideas pertaining to the actual implementation of beamforming; however, most have some basic similarities. The vast majority of these implementations are based on delay and sum beamforming - which is the idea of inserting different time delays at each driver so that a single directed wave-front can be created and controlled by changing the time delays and thus how these waves sum in the field. Beamforming allows an array of receivers or transmitters to function as a single larger receiver or transmitter (Greenwood, 2011). The receiver or transmitter can be changed by simply changing the beamforming pattern, which allows unique signal control opportunities (Greenwood, 2011). Furthermore, the beamforming image is created by these phase shifts and integrated over the array (receivers or transmitters) being used (Simonetti & Huang, 2009). However, this is just a conceptual idea of how beamforming works. In reality there are much more complex mathematical ideas at play. A key idea is that of Minimum Variance, proposed for use in Photoacoustic Imaging by Park, Karpiouk, Aglyamov, and Emelianov (2008), which aims to reduce off-axis signals and therefore increase the accuracy of the signals being transmitted. Additionally, Lagrange multipliers and covariance matrices can be used to perform spatial smoothing (Park, Karpiouk, Aglyamov, & Emelianov, 2008), although with a photoacoustic image, which is not the acoustic image that this research aims to address. In another instance beamforming was used to measure depth in a horizontal waveguide; the experimenters attempted to maximize the signal to noise ratio (Hinich, 1977). To find sound pressures over the area of the field, the wave equation was solved; then, using statistical analysis and Gaussian distributions, changing wave values over time could be predicted and calculated (Hinich, 1977). Although there are many positive reviews of beamforming, an article on the creation of sound bullets deplores beamforming for being reliant on actuators and its use of cumbersome and application specific drivers (Spadoni, Daraio, & Freund, 2010).</p>

	<p>The methods described to implement beamforming so far have been similar because all are variations of delay and sum beamforming. It has proved successful because uses of such beamforming abound. For instance, beamforming is common in the radar and communication systems of military aircraft to accurately place Radio Frequency (RF) waves in new Active Electronically Scanned Arrays (Manz, 2012). Variations of these arrays have evolved themselves into radar inferometers, which are used to map landscapes below the array and utilize beamforming to aid with these measurements (Deller et al., 2011). Other terms used are, beam shaping or cross-strapping; both refer to beamforming and its use in the communication satellite INTELSAT VI (Pollack & Weiss, 1984). Beamforming was also used to determine the directionality of surface-generated noise in an upward refracting ocean by measuring the wave in each plane (Buckingham, 1994). In another medical study, a MEG (magneto encephalography) used beamforming to produce rich frequency information (Hagan et al., 2009). These examples show the relevance of beamforming to both civilian and military applications.</p>
	

	<a name = "simulations"></a>
    <hr class="featurette-divider">
	<h2> Simulations </h2>
	<p> The purpose of simulation was to see if there was a difference in directionality and magnitude between three cases: point and sum beamforming, delay and sum beamforming, and no beamforming. Point and sum beamforming uses the Pythagorean theorem to calculate the delay based on the dimensions of the speaker array, the distance to the user and the speed of sound. On the other hand, delay and sum beamforming creates a wave-front where each delay is a multiple of the first (x, 2x, 3x ... etc.) Finally, a system with no beamforming has no delays or summing built in, each wave is simultaneously driven out of each speaker.</p>
	<br>
	<h4> Simulation Design </h4>
	<p> To begin the simulations, a 30 inch by inch field was constructed in simulation to mimic the field over which the waves would propagate. The five distinct sound waves, modeled as sine waves, were then created with phase shifts in the x-direction (-4, -2, 0, 2, 4) based on the position of each transmitter modeled. The delay of each wave creates beamforming by implementing phase shifts in the y-direction. Omega controls the period of the function, which in turn is dependent on the frequency. The signal phase_shift controls the phase shift of every signal together, which is used to measure signal strength over time. An example equation of all three is below: </p>
	<br>
	<p>Each of the equations above corresponds to one of the three different types of beamforming tested. The main difference between Equations 1, 2 and 3 is the term modifying the y-coordinate - which is the difference in distance that each wave has to travel and represents the delay. By changing omega it was possible to sweep through the frequency domain and by changing (phase_shift) it was possible to sweep across the time domain to find the total magnitude of the wave.</p>
	<p>These variables made it feasible to sweep across both time and frequency domains to test each method of beamforming in MATLAB, but a method was needed to determine which signal was the most powerful and had the most separation between channels. To find this, the width of the beam was measured in each case. First, an algorithm was written to find the maximum value in the field, which became z_max. In order to find the point where the sound was no longer audible, which leads to the width of the beam, an edge finder method was developed based on
5 the logarithmic properties of hearing and decibels. Solving Equation 4 leads to Equation 5, which gives a formula for sound in decibels versus magnitude of signal. Sound was considered inaudible when it was half of the magnitude of the maximum signal as seen in Equation 6. </p>
	
	<br>
	<div class = "container">
	<h4> Near-Field Simulation Result Images </h4>
	<div class="row">
  	  	<div class="col-6 col-sm-6 col-lg-4">
    		<div class="thumbnail">
				<a href = "website/pics/simulation_results/near_inst.png">
			    <img src="website/pics/simulation_results/near_inst.png" alt="">
		  		</a>
				<h4>Near-Field Instantaneous Results </h4>
      		    <p> This graph shows a top view of the instantaneos waveforms created using near-field beamforming. </p>
   		 	</div>
		</div>
  	  	<div class="col-6 col-sm-6 col-lg-4">
    		<div class="thumbnail">
				<a href = "website/pics/simulation_results/near_sum.png">
			  <img src="website/pics/simulation_results/near_sum.png" alt="">
		  </a>
				<h4>Near-Field Summed Results</h4>
      		    <p> This graph shows a top view of the summed waveforms created using near-field beamforming.</p>
   		 	</div>
		</div>
  	  	<div class="col-6 col-sm-6 col-lg-4">
    		<div class="thumbnail">
				<a href = "website/pics/simulation_results/near_graph.png">
			  <img src="website/pics/simulation_results/near_graph.png" alt="">
		  </a>
				<h4>Near-Field Frequency Sweep</h4>
      		    <p> This graph shows the performance of the system (measured by channel separation) across frequencies.</p>
   		 	</div>
		</div>
	</div>

	<br>
	<h4> Far-Field Simulation Result Images </h4>
	<div class="row">
  	  	<div class="col-6 col-sm-6 col-lg-4">
    		<div class="thumbnail">
				<a href = "website/pics/simulation_results/far_inst.png">
			    <img src="website/pics/simulation_results/far_inst.png" alt="">
		  		</a>
				<h4>Far-Field Instantaneous Results </h4>
      		    <p> This graph shows a top view of the instantaneos waveforms created using far-field beamforming. </p>
   		 	</div>
		</div>
  	  	<div class="col-6 col-sm-6 col-lg-4">
    		<div class="thumbnail">
				<a href = "website/pics/simulation_results/far_sum.png">
			  <img src="website/pics/simulation_results/far_sum.png" alt="">
		  </a>
				<h4>Far-Field Summed Results</h4>
      		    <p> This graph shows a top view of the summed waveforms created using far-field beamforming.</p>
   		 	</div>
		</div>
  	  	<div class="col-6 col-sm-6 col-lg-4">
    		<div class="thumbnail">
				<a href = "website/pics/simulation_results/far_graph.png">
			  <img src="website/pics/simulation_results/far_graph.png" alt="">
		  </a>
				<h4>Far-Field Frequency Sweep</h4>
      		    <p> This graph shows the performance of the system (measured by channel separation) across frequencies.</p>
   		 	</div>
		</div>
	</div>
	
	<br>
	<h4> No Beamforming Simulation Result Images </h4>
	<div class="row">
  	  	<div class="col-6 col-sm-6 col-lg-4">
    		<div class="thumbnail">
				<a href = "website/pics/simulation_results/no_inst.png">
			    <img src="website/pics/simulation_results/no_inst.png" alt="">
		  		</a>
				<h4> No Beamforming Instantaneous Results </h4>
      		    <p> This graph shows a top view of the instantaneos waveforms created using no beamforming. </p>
   		 	</div>
		</div>
  	  	<div class="col-6 col-sm-6 col-lg-4">
    		<div class="thumbnail">
				<a href = "website/pics/simulation_results/no_sum.png">
			  <img src="website/pics/simulation_results/no_sum.png" alt="">
		  </a>
				<h4>No Beamforming Summed Results</h4>
      		    <p> This graph shows a top view of the summed waveforms created using no beamforming.</p>
   		 	</div>
		</div>
  	  	<div class="col-6 col-sm-6 col-lg-4">
    		<div class="thumbnail">
				<a href = "website/pics/simulation_results/no_graph.png">
			  <img src="website/pics/simulation_results/no_graph.png" alt="">
		  </a>
				<h4>No Beamforming Frequency Sweep</h4>
      		    <p> This graph shows the performance of the system (measured by channel separation) across frequencies.</p>
   		 	</div>
		</div>
	</div>
	</div>

	<a name = "fpga_design"></a>
    <hr class="featurette-divider">
	<h2> FPGA Designs </h2>
	<p>Although the point and sum methodology had been developed in simulation, it also required testing in hardware to confirm its functionality for acoustic applications. The processing hardware (FPGA design) was written in Very High Speed Integrated Circuit Hardware Description Language (VHDL) with four main processes: the delay calculator, clock division, data processing and an output selector. A block diagram is shown in Figure 12.</p>
	
	<p>To create a clock with a period of 1 us (named us_clock), a clock division was used. First a generic (name for constant in VHDL) was created with the predetermined division ratio of the system clock to the us_clock. This was accomplished by counting rising edges of the system clock (i_clock) until it equaled the generic, then inverting the current state of us_clock. There was also a reset command that set the edge counter back to zero and us_clock to zero if needed.</p>
	
	<p>Based on the delays calculated, it was possible to process the input data into numbers ready for output and amplification. A shift register was created out of logic vectors (8 bit numbers for sound) to hold each sound sample. Every 23 μs (period of a 44,100 Hz clock rounded to nearest integer) a new sample was fed into the shift register and the other values shifted down one. At the same time, a counter was running from 0 to 115 to implement the near- field delays discussed above. When it reached a desired delay value that point in the shift register was stored to the appropriate speaker channel for output. This process was repeated for both the left and right channels separately. However, an output method was needed to address the lack of input and output pins on the Nexys 3 Board (Spartan 6 FPGA) used for experimentation.
Because there were not sufficient input and output pins on the FPGA to perform all operations in parallel, the data going out of the FGPA needed to be multiplexed. This was achieved in 5 us by quickly enabling each channel individually based on a counter triggered by the rising edge of the us_clock. When each channel was enabled, the Digital to Analog Converter (DAC) sampled that channel via the o_dataout signal. Each o_dataout channel added a left and right channel together for instance data_l_0 and data_r_4 were summed together and data_l_2 and data_r_2 were also summed together. This 8 bit digital audio was then converted to analog through a MX7224 DAC then amplified using two LF411 operational amplifiers. Finally, using TIP 41 and TIP 42 transistors, the current was amplified to drive an Aurasound NSW1-205-8A speaker. This audio chain was created five times, one for each channel used.</p>
	<br>
	<h4> FPGA Block Diagrams </h4>
	<div class = "container">
	<div class="row">
  	  	<div class="col-6 col-sm-6 col-lg-4">
    		<div class="thumbnail">
				<a href = "website/pics/system.jpg">
			  <img src="website/pics/system.jpg" alt="">
		  		</a>
				<h4>System Block Diagram </h4>
      		    <p>This shows the over view of the entire system developed. </p>
   		 	</div>
		</div>
  	  	<div class="col-6 col-sm-6 col-lg-4">
    		<div class="thumbnail">
				<a href = "website/pics/vhdl_code.jpg">
			  <img src="website/pics/vhdl_code.jpg" alt="">
		  </a>
				<h4>VHDL Code Block Diagram</h4>
      		    <p> This diagram shows the general interaction of the different FPGA processes written.</p>
   		 	</div>
		</div>
	</div>
	</div>
		
	
	<a name = "design_files"></a>
    <hr class="featurette-divider">
	<h2> Design Files </h2>
	<p>There are simulation files (written in MATLAB) and FPGA design files (written in VHDL). Both are hosted on GitHub and linked below. Prior experimentation files and schematics can be found on my earlier website.</p>
	<p> <a href = "https://github.com/srohrer32/beamformer"> Design files </a></p>
	<p> <a href = "http://www.srohrer.x10host.com/home/nearfield.html"> Old Designs </a></p>
    <hr class="featurette-divider">
	
	<!-- ***************** Footer ********************** -->
    <footer>
      <p class="pull-right"><a href="#top">Back to top</a></p>
      <p>2013 -- Please note that all materials and ideas here were created by Samuel Rohrer and are solely intended for educational purposes. Website built using Twitter Bootstrap.</p>
    </footer>
	</div>

    <script src="website/js/bootstrap.js"></script>
	<script>
	$(document).ready(function() {
	  $('.nav').onePageNav();
	});
	</script>

</body>
</html>